\documentclass[12pt,letterpaper]{article}
\usepackage[a4paper, top=0.75in, bottom=1in, left=0.7in, right=0.7in]{geometry} 
\usepackage{amsmath, amsfonts, amssymb, amsthm, amscd} % Combined math packages
\usepackage{tikz}     % For drawing matrices
\usepackage{listings, pythontex} % For code snippets
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{float} % To control figure placement

\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.2in} % Increased spacing for readability

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%% CHANGE HERE %%%%%%%%%%%%%%%%%%%%
\newcommand\course{STA4724}
\newcommand\semester{Spring 2025}
\newcommand\hwnumber{3}                 % <-- ASSIGNMENT #
\newcommand\NetIDa{Albert Jojo}          % <-- YOUR NAME
\newcommand\NetIDb{5390131}              % <-- STUDENT ID #
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%%
\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa \\ \NetIDb} % Combined name and ID in the left header
\chead{\textbf{\Large Homework \hwnumber}}
\rhead{\course \\ \semester}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\sloppy

\section*{Question 1}
(a). Image segmentation usually serves as the pre-processing before pattern recognition, feature extrac-
tion, and compression of the image. Explore using the K-Means clustering algorithm to read the
image provided and cluster different regions (K = 2, 3, 4, 5, 6) of the image.
\vspace{-0.2in}
\subsection*{1a}
The code for the k-means algorithm I used is given below:
\begin{lstlisting}[language=Python, breaklines=true, columns=flexible, frame=single]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import cv2

def read_image(image_path):
    image = cv2.imread(image_path)
    assert image is not None, f"Error: Unable to load image at {image_path}"

    # Convert BGR to RGB
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    return image_rgb

def process_img(img_rgb):
    # Reshape image to 2D array of pixels for clustering
    pixels = img_rgb.reshape(-1, 3)
    return pixels

def k_means(img, pixels, k):
    # Apply K-Means algorithm
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(pixels)

    # Get segmented pixels by mapping each pixel to its cluster center
    segmented_pixels = kmeans.cluster_centers_[kmeans.labels_]

    # Reshape back to the original image shape
    segmented_img = segmented_pixels.reshape(img.shape).astype(np.uint8)

    return segmented_img

def plt_imgs(*images):
    titles = [
        "Original Image", "Clustered Image (K=2)", "Clustered Image (K=3)",
        "Clustered Image (K=4)", "Clustered Image (K=5)", "Clustered Image (K=6)"
    ]

    fig, axs = plt.subplots(2, 3, figsize=(10, 6))

    for i, ax in enumerate(axs.flat):
        ax.imshow(images[i])
        ax.set_title(titles[i])
        if i != 0:
            ax.axis("off")

    plt.tight_layout()
    plt.show()

def main():
    image_path = "/Users/aj/UCF_Courses/Spring_2025/STA4724/hw/HW3/image.jpg"
    image_rgb = read_image(image_path)
    pixels = process_img(image_rgb)

    # Apply K-Means clustering with different K values
    kmeans_2 = k_means(image_rgb, pixels, 2)
    kmeans_3 = k_means(image_rgb, pixels, 3)
    kmeans_4 = k_means(image_rgb, pixels, 4)
    kmeans_5 = k_means(image_rgb, pixels, 5)
    kmeans_6 = k_means(image_rgb, pixels, 6)

    # Display the results
    plt_imgs(image_rgb, kmeans_2, kmeans_3, kmeans_4, kmeans_5, kmeans_6)

if __name__ == "__main__":
    main()
\end{lstlisting}
\pagebreak
\subsubsection*{Output}
The output for K-means is given below:
\begin{figure}[H]  % 'H' forces the figure to be placed here
    \centering
    \includegraphics[width=0.85\textwidth]{k_means_fig.png}
    \caption{KMeans Output}
    \label{Fig:1}
\end{figure}

\vspace{-0.2in}  % Optional: Adjust space after the figure to reduce gap
\pagebreak
\subsection*{1b}
(b) Try to use the Guassian Mixture Model (GMM) to cluster different regions(K=2,3,4,5,6) of the same image.
\begin{lstlisting}[language=Python, breaklines=true, columns=flexible, frame=single]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
import cv2

def read_image(image_path):
    image = cv2.imread(image_path)
    assert image is not None, f"Error: Unable to load image at {image_path}"

    # Convert BGR to RGB
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    return image_rgb

def gmm(img_rgb, n_components):
    pixels = img_rgb.reshape(-1, 3)

    gmm = GaussianMixture(n_components=n_components, random_state=42, n_init=10)
    gmm.fit(pixels)

    # Get the predicted labels for each pixel
    labels = gmm.predict(pixels)

    # Assign the mean color to each pixel based on the predicted label
    segmented_pixels = gmm.means_[labels]

    # Reshape the segmented pixels to the original image shape
    segmented_img = segmented_pixels.reshape(img_rgb.shape)

    # Ensure pixel values are within the valid range
    segmented_img = np.clip(segmented_img, 0, 255).astype(np.uint8)

    return segmented_img

def plt_imgs(*images):
    titles = [
        "Original Image", "Clustered Image (K=2)", "Clustered Image (K=3)",
        "Clustered Image (K=4)", "Clustered Image (K=5)", "Clustered Image (K=6)"
    ]

    fig, axs = plt.subplots(2, 3, figsize=(10, 6))

    for i, ax in enumerate(axs.flat):
        ax.imshow(images[i])
        ax.set_title(titles[i])
        if i != 0:
            ax.axis("off")

    plt.tight_layout()
    plt.show()

def main():
    image_path = "/Users/aj/UCF_Courses/Spring_2025/STA4724/hw/HW3/image.jpg"
    image_rgb = read_image(image_path)
    
    gmm_2 = gmm(img_rgb=image_rgb, n_components=2)
    gmm_3 = gmm(img_rgb=image_rgb, n_components=3)
    gmm_4 = gmm(img_rgb=image_rgb, n_components=4)
    gmm_5 = gmm(img_rgb=image_rgb, n_components=5)
    gmm_6 = gmm(img_rgb=image_rgb, n_components=6)

    plt_imgs(image_rgb, gmm_2, gmm_3, gmm_4, gmm_5, gmm_6)

if __name__ == "__main__":
    main()
\end{lstlisting}
\pagebreak
\subsubsection*{Output}
The output for Guassian Mixture Model is given below:
\begin{figure}[H]  % 'H' forces the figure to be placed here
    \centering
    \includegraphics[width=0.85\textwidth]{gaussian_mixture_fig.png}
    \caption{Guassian Mixture Output}
    \label{Fig:2}
\end{figure}

\vspace{-0.2in}  % Optional: Adjust space after the figure to reduce gap
\pagebreak
\section*{Question 2}
\begin{itemize}
    \item[(a)] Explain why the agglomerative clustering algorithm is not applicable to the image segmentation practice above?
    \vspace{0.2in}
    \\ Agglomerative clustering is not applicable to image segementation because the algorithm starts by treating each pixel as an indivual 
    cluster and then merges them iteratively. This approach is compuationally ineffective and costly, because image contains a massive number of pixels,
    leading to high time and memory complexity. Furthermore, agglomerative clustering does not inherently capture the spatial relationships between pixels, which is
    essential for meaningful segmentation.Thus, agglomerative clustering is not applicable to image segmentation.

    \item[(b)] What is the difference between agglomerative clustering algorithm and the divisive clustering algorithm?
    \vspace{0.2in}
    \\ Agglomerative Clustering and Divisive Clustering are two types of hierarchical clustering methods that differ in their approach to clustering. Agglomerative Clustering uses a \textbf{Bottom-Up Approach}, meaning it starts with each data point as its own individual cluster. Then, it repeatedly merges the closest pairs of clusters based on some similarity or distance measure, such as Euclidean distance, until all the points are in a single cluster. This process builds a hierarchy of clusters by progressively merging them, and the resulting dendrogram helps visualize the merging process. 
    \vspace{0.2in}
    \\ Divisive Clustering, on the other hand, uses a \textbf{Top-Down Approach}. It begins with all data points in a single cluster and recursively splits the cluster into two smaller clusters based on the greatest dissimilarity between them. This process continues until each data point is isolated in its own cluster. Divisive Clustering is typically more computationally expensive than Agglomerative Clustering because it requires evaluating all possible splits at each step. 
    
    
    \item[(c)] In the equation of the Davies-Bouldin Index, \(V_{DB} = \frac{1}{K}\sum_{k=1}^{K} \max_{l\neq k} S_{kl}\), what is \(S_{kl}\)?
    \vspace{0.2in}
    \\ In the equation of Davies-Bouldin Index: \(\mathbf{S_{kl}}\) represents the measure of separation between the two clusters k and l.
    \item[(d)] Elaborate the following equation of the partition coefficient index:

    \[
    V_{PC} = \frac{1}{N} \sum_{k,n} P(k|X_n)^2
    \]

    \noindent The partition coefficient index (\(V_{PC}\)) is a measure used to evaluate the fuzziness of a clustering result. It is defined as the average squared membership values of data points across all clusters. 
    \noindent The partition coefficient index ranges from \( \frac{1}{K} \) to 1. A higher \(V_{PC}\) indicates more distinct clustering, while a lower value suggests a more soft partitioning.

    \end{itemize}

\end{document}
